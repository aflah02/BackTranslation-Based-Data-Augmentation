{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BacktranslationUsingPivotLanguage.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyME7MvRv0NYuLyhq9UYjvyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/BackTranslation-Based-Data-Augmentation/blob/main/BacktranslationUsingPivotLanguage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QpgTCThXR6NY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51f3c51-e4a1-4e6c-99f2-e3b297f3259b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-5.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting lxml>=4.8.0\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 6.8 MB/s \n",
            "\u001b[?25hCollecting pathos>=0.2.8\n",
            "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting loguru>=0.6.0\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting requests>=2.27.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting PyExecJS>=1.5.1\n",
            "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.3.4)\n",
            "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.8->translators) (0.70.12.2)\n",
            "Collecting ppft>=1.6.6.4\n",
            "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting pox>=0.3.0\n",
            "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from ppft>=1.6.6.4->pathos>=0.2.8->translators) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.27.1->translators) (2.10)\n",
            "Building wheels for collected packages: PyExecJS\n",
            "  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14598 sha256=a1059f51e1c89c3a3cc509347e8627f4e654b942fa7fe9283e14d06f07e86701\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/ee/03/da5c0b4a8c13362beeb844eb913bbe58a89bde1de2b9157007\n",
            "Successfully built PyExecJS\n",
            "Installing collected packages: ppft, pox, requests, PyExecJS, pathos, lxml, loguru, translators\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyExecJS-1.5.1 loguru-0.6.0 lxml-4.8.0 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 requests-2.27.1 translators-5.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using United States server backend.\n"
          ]
        }
      ],
      "source": [
        "!pip install translators\n",
        "import pandas as pd\n",
        "import translators as ts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translator_constructor(api):\n",
        "    if api == 'google':\n",
        "        return ts.google\n",
        "    elif api == 'bing':\n",
        "        return ts.bing\n",
        "    elif api == 'baidu':\n",
        "        return ts.baidu\n",
        "    elif api == 'sogou':\n",
        "        return ts.sogou\n",
        "    elif api == 'youdao':\n",
        "        return ts.youdao\n",
        "    elif api == 'tencent':\n",
        "        return ts.tencent\n",
        "    elif api == 'alibaba':\n",
        "        return ts.alibaba\n",
        "    else:\n",
        "        raise NotImplementedError(f'{api} translator is not realised!')"
      ],
      "metadata": {
        "id": "bcc4aGfx8COf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Download"
      ],
      "metadata": {
        "id": "9XRLn4QTSX6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/dev.tsv"
      ],
      "metadata": {
        "id": "UV8YG0_9SXiG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Building Function"
      ],
      "metadata": {
        "id": "JB_foFny7hAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buildTrainTestDatasets(doAug, NumberOfTrainingSamplesTouse, pivotLanguageCode, API, currentLanguageCode):\n",
        "  df_train = pd.read_csv('train.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "  df_dev = pd.read_csv('dev.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "  df_test = pd.read_csv('test.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "\n",
        "  df_train = pd.concat([df_train, df_dev])\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_train = df_train.loc[:, [\"Label\", \"Sentence\"]]\n",
        "  df_test = df_test.loc[:, [\"Label\", \"Sentence\"]]\n",
        "  if (NumberOfTrainingSamplesTouse > len(df_train)):\n",
        "    raise Exception(\"More Samples Asked For Than Present\")\n",
        "\n",
        "  df_train = df_train.iloc[:NumberOfTrainingSamplesTouse]\n",
        "  if (doAug):\n",
        "    convertedSentences = []\n",
        "    originalSentences = df_train[\"Sentence\"].to_list()\n",
        "    originalLabels = df_train[\"Label\"].to_list()\n",
        "    translateFunction = translator_constructor(API)\n",
        "    for sentence in originalSentences:\n",
        "      convertedSentences.append(translateFunction(translateFunction(sentence, currentLanguageCode, pivotLanguageCode), pivotLanguageCode, currentLanguageCode))\n",
        "    newSentenceSet = originalSentences + convertedSentences\n",
        "    newLabelSet = originalLabels + originalLabels\n",
        "    df_train = pd.DataFrame(\n",
        "        {'Label': newSentenceSet,\n",
        "        'Sentence': newLabelSet\n",
        "         })\n",
        "  return df_train, df_test"
      ],
      "metadata": {
        "id": "gv9ncJ1aSXY7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = buildTrainTestDatasets(True, 100, 'es', 'google', 'en')"
      ],
      "metadata": {
        "id": "_36euix4aNtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1b6IU-ms7tHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}